{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain\n",
        "LangChain is a powerful framework designed to streamline and enhance the development of applications using Large Language Models (LLMs).\n",
        "\n",
        "# overview\n",
        "This notebook will introduce you to some of the core concepts and components of LangChain, including:\n",
        "- **LLM Wrappers**: Learn how to use LangChain to interact with different large language models seamlessly.\n",
        "- **Prompt Templates**: Discover how to create and manage reusable templates for generating consistent and effective prompts.\n",
        "- **Chaining**: Understand how to chain multiple LLM calls together to build more complex and capable systems.\n",
        "- **Structuring**: Formatting LLM outputs into a certain structure\n",
        "- **Text Splitting**: Explore techniques for splitting text into manageable chunks for processing.\n",
        "- **Embedding**: Get insights into generating and utilizing embeddings for various natural language processing tasks.\n",
        "- **Retrieval**: Dive into methods for retrieving relevant information from large text corpora using embeddings and other techniques."
      ],
      "metadata": {
        "id": "YmIFEyHOzvfn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package installs\n"
      ],
      "metadata": {
        "id": "gpGeimUC00qd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2-pgPAX_I7j"
      },
      "outputs": [],
      "source": [
        "# install required packages; this may take some minutes; ignore dependency warnings it should work anyway\n",
        "%pip install openai\n",
        "%pip install langchain\n",
        "%pip install langchain-openai\n",
        "%pip install langchain-community\n",
        "%pip install pypdf\n",
        "%pip install tiktoken\n",
        "%pip install chromadb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key setup\n",
        "\n",
        "Let's set up the openai API key"
      ],
      "metadata": {
        "id": "4afzLb2H1CHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "openai_api_key = 'API_KEY'\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ],
      "metadata": {
        "id": "dtcAZNQJ1GWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LLM Wrappers**\n",
        "\n",
        "We've seen in the API hands-on how to use the openAI api to call gpt.\n",
        "Here we see another way to utilize LLMs using langchain.\n",
        "Here Langchain does all the heavy lifting for us in the 'backend'"
      ],
      "metadata": {
        "id": "8Eb-tLyK1O8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0, max_tokens=20,)\n",
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "output_message = model.invoke(messages)\n",
        "print(output_message)"
      ],
      "metadata": {
        "id": "r2iF7Qi41PgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instead of using Tuples (\"sender\",\"message\") we can use langchain objects"
      ],
      "metadata": {
        "id": "YR1-OtpI1Sib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"YOUR SYSTEM PROMPT HERE\"),\n",
        "    HumanMessage(content=\"YOUR USER PROMPT HERE\")\n",
        "]\n",
        "\n",
        "response = model.invoke(messages).content"
      ],
      "metadata": {
        "id": "hplmf_Wn1TaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates\n",
        "\n",
        "Now this is already great. But it doesn't scale well.\n",
        "Everytime we need to ask a question we have to rewrite the prompt.\n",
        "\n",
        "And this is where prompt templates come in for.\n",
        "Assume we have the following system message\n",
        "\n",
        "`You are an expert on machine learning and data science and your goal is to help studnets learn about different topics. Explain the topic they ask about like they were 10 years old. Assume they have no knowledge of that subject. Make sure to answer the question in French`\n",
        "\n",
        "What if for a different use case you now want the LLM to answer in German? or English or any other language? it would be easier to not have to do another prompt right? What if you want it to be an expert in a differnt field. And so on.\n",
        "\n"
      ],
      "metadata": {
        "id": "cSSn-oOZ1Vvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        SystemMessagePromptTemplate.from_template(\"You are an expert on {field} and your goal is to help studnets learn about different topics. Explain the topic they ask about like they were 10 years old. Assume they have no knowledge of that subject. Make sure to answer the question in {language}\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "prompt.invoke({\n",
        "    \"field\": \"data science and machine learning\",\n",
        "    \"language\": \"english\",\n",
        "    \"input\": \"Large Language Models\"\n",
        "})"
      ],
      "metadata": {
        "id": "GGRuq2nh1WF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chaining\n",
        "\n",
        "Now that we have dynamically changing prompts we want to pass that to our models. We can invoke to get the prompt then invoke the model to get the response.\n",
        "But luckily Langchain can do all of the work for us by using chains.\n",
        "\n",
        "Chains are constructed by using `|` between different parts of the chain for example if the output of some llm_model `A` needs to go into another llm_model `B` (or the same one) we can build the chain `A | B`"
      ],
      "metadata": {
        "id": "0CiJY_Tt1ZVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "basic_chain = prompt | model\n",
        "\n",
        "# now to invoke the entire chain\n",
        "# have fun making up some cool prompts\n",
        "chain_output = basic_chain.invoke({\n",
        "    \"field\": \"ENTER SOME FIELD (computer science)\",\n",
        "    \"language\": \"ENTER SOME LANGUAGE (german)\",\n",
        "    \"input\": \"ENTER A TOPIC\"\n",
        "})\n",
        "print(chain_output.content)"
      ],
      "metadata": {
        "id": "hOyQx8wL1ZJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How about we also stop having to do `output.content`\n",
        "\n",
        "Let's add another node to the chain and use a parser"
      ],
      "metadata": {
        "id": "6RE1WJvk5-Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "parser_chain = basic_chain | parser # you can also recreate this from scratch using prompt | model | parser ..."
      ],
      "metadata": {
        "id": "TcoK5mCl6HLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain_output = parser_chain.invoke({\n",
        "    \"field\": \"ENTER SOME FIELD (computer science)\",\n",
        "    \"language\": \"ENTER SOME LANGUAGE (german)\",\n",
        "    \"input\": \"ENTER A TOPIC\"\n",
        "})\n",
        "print(chain_output)"
      ],
      "metadata": {
        "id": "n4DGNvIc6P_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structuring\n",
        "Now that we have a nice chain with text output, it might also be way nicer to structure the output into some format where we can automate its use.\n",
        "\n",
        "For example, assume hundreds of people used the previous chain. And you're tasked with analyzing the output. It would be easier if you can create a csv file where you get the field language and input among other information.\n",
        "\n",
        "Since you know the prompt template you can get these information by code. But what if you needed extra information.\n",
        "\n",
        "Well Langchain can provide that as well\n"
      ],
      "metadata": {
        "id": "XlMutfAO8coJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start by creating a class that explains the expected structure\n",
        "\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from typing import Optional\n",
        "\n",
        "class ExplainationStructure(BaseModel):\n",
        "  \"\"\"Structure of the explaination\"\"\"\n",
        "  field: str = Field(description=\"Field of expertise required to understand the topic\")\n",
        "  language: str = Field(description=\"Language of the text\")\n",
        "  text: str = Field(description=\"The previous entire text\")\n",
        "  required_knowledge: Optional[str] = Field(description=\"Knowledge required to understand the answer\")\n",
        "  extra_knowledge: str = Field(description=\"Extra information for the user to read, and related topics\")\n",
        "  # + any other outputs you expect from the LLM\n",
        "\n",
        "# create the new model that will structure our outputs for us\n",
        "structured_model = model.with_structured_output(ExplainationStructure)   # can re-use the model defined above or create a new one"
      ],
      "metadata": {
        "id": "hiSOfcLe88z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can use this model by itself using structured_model.invoke(\"What can you tell me about LLMs?\")\n",
        "# but its more interesting to see it as part of our chain. Let's add it as a node to our parser chain\n",
        "\n",
        "structuring_chain = parser_chain | structured_model\n",
        "\n",
        "# then we can run it as we did before\n",
        "chain_output = structuring_chain.invoke({\n",
        "    \"field\": \"Economics\",\n",
        "    \"language\": \"English\",\n",
        "    \"input\": \"Buisness Model\"\n",
        "})\n",
        "print(chain_output)"
      ],
      "metadata": {
        "id": "cC-hsJcQACJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Splitting\n",
        "Sometimes the text input/output is too large. And in order to work with it you might need to split it to different chunks.\n",
        "Langchain actually helps us with this too"
      ],
      "metadata": {
        "id": "6I628Yx11cbo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "long_text = \"\"\"Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
        "\n",
        "When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.\n",
        "\n",
        "At a high level, text splitters work as following:\n",
        "\n",
        "Split the text up into small, semantically meaningful chunks (often sentences).\n",
        "Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function).\n",
        "Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).\n",
        "That means there are two different axes along which you can customize your text splitter:\n",
        "\n",
        "How the text is split\n",
        "How the chunk size is measured\n",
        "For specifics on how to use text splitters, see the relevant how-to guides here.\"\"\""
      ],
      "metadata": {
        "id": "azw3XsLy9CcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes the text is part of a PDF or a text file. We can also read from that using the following code snippet"
      ],
      "metadata": {
        "id": "ZhbY94GS9Gu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = \"PATH_TO_PDF.pdf\"\n",
        "loader = PyPDFLoader(pdf_path)\n",
        "documents = loader.load()\n",
        "\n",
        "# once the pdf loads its divided into different pages. We can loop over them and create a string variable for all the text\n",
        "long_text = \"\"\n",
        "for page in documents[:2]:\n",
        "    long_text += page.page_content + \"\\n\\n\""
      ],
      "metadata": {
        "id": "1N_jDxk39F4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
        "\n",
        "text_chunks = text_splitter.create_documents([long_text])\n",
        "\n",
        "print(text_chunks)"
      ],
      "metadata": {
        "id": "bwGiUudX1cTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings and Vector Spaces\n",
        "\n",
        "Embeddings are a vector representation of words. Basically a list of numbers for every word.\n",
        "\n",
        "This is useful because similar words can now be put together in the vector space. For example we can assume all colors are close together and all animals are close together. We can then have Fish and Aquarium be close because they also relate to one another.\n",
        "\n",
        "\n",
        "We can use langchain to embbed large documents into vector space.\n",
        "This allows us to find relevant information to our prompts\n"
      ],
      "metadata": {
        "id": "ftKqnc3D1fio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "\n",
        "# we can then use the embedding on a text query\n",
        "vector = embedding_model.embed_query(\"This is an embedding\")\n",
        "print(vector)"
      ],
      "metadata": {
        "id": "LwOWfpTV1faD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VectorStore\n",
        "\n",
        "Now that we have embeddings for our text we need to store the output somewhere. Langchain already offers integration with multiple databases.\n",
        "\n",
        "For this exercise we will be using Chroma.\n",
        "This is an in-memory database meaning the output is stored on your machine and used locally"
      ],
      "metadata": {
        "id": "sxZ7BE_X1iPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's create a directory to store our data\n",
        "vectorstore_path = \"chroma/\"\n",
        "try:\n",
        "  os.mkdir(vectorstore_path)\n",
        "except FileExistsError:\n",
        "  !rm -rf chroma/  # remove old database files if any\n",
        "  os.mkdir(vectorstore_path)\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=text_chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=vectorstore_path\n",
        ")"
      ],
      "metadata": {
        "id": "RVa67W0x1gtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Retrieval\n",
        "\n",
        "Now that our embeddings are saved in a database (in-memory for our case) we would like to retrieve information with similar text.\n",
        "This is then useful for QA systems that use RAG\n",
        "\n"
      ],
      "metadata": {
        "id": "DVEtUXYF1pUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How does text splitting work?\"\n",
        "docs = vectordb.similarity_search(question,k=3)\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "lmBf2YH11lbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how langchain does it for us"
      ],
      "metadata": {
        "id": "Knt690dv8OSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    model,\n",
        "    retriever=vectordb.as_retriever()\n",
        ")\n",
        "\n",
        "result = qa_chain({\"query\": question})\n",
        "result[\"result\"]"
      ],
      "metadata": {
        "id": "ybuqAXD68MtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agents\n",
        "\n",
        "Finally we talked about Langchain containing agents that run code and do complex task.\n",
        "\n",
        "Let's take a look at a basic langchain agent that forwards your prompt into the python math library and calculates it."
      ],
      "metadata": {
        "id": "8OObHx0UFX--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMMathChain\n",
        "\n",
        "llm_math = LLMMathChain.from_llm(model)"
      ],
      "metadata": {
        "id": "KnzpYQtGFlFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    (\"human\", \"What is the square root of 5 multiplied by pi\"),\n",
        "]\n",
        "output_message = llm_math.invoke(messages)\n",
        "output_message"
      ],
      "metadata": {
        "id": "_I_zxxtGFnBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is so much more you can do with agents.\n",
        "Feel free to read [this](https://python.langchain.com/v0.2/docs/how_to/function_calling//) article about function calling to learn more"
      ],
      "metadata": {
        "id": "hx71SSonBQ4D"
      }
    }
  ]
}