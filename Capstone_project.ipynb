{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUc5ebkGPA5I"
      },
      "source": [
        "# Capstone Project\n",
        "You've learned so much from the previous two hands-on exercises already.\n",
        "\n",
        "For this capstone project I would like to put all your skill to the test.\n",
        "We will be using everything we've learned to create an Agent, that when asked a question, finds out the proper wikipedia page, scrapes that pages, and uses RAG to answer the question.\n",
        "We're going to utilize the concepts of, API calls and Webscrapping, Langchain tools and function calling, Text splitting, Embedding and vector similarity search and Retrieval based Q&A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2aZhWwQQG6R"
      },
      "source": [
        "# Installations\n",
        "Let's install all required packages for this notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCweoyZk_PXd"
      },
      "outputs": [],
      "source": [
        "# install required packages; this may take some minutes; ignore dependency warnings it should work anyway\n",
        "%pip install openai\n",
        "%pip install langchain\n",
        "%pip install langchain-openai\n",
        "%pip install langchain-community\n",
        "%pip install pypdf\n",
        "%pip install tiktoken\n",
        "%pip install chromadb\n",
        "%pip install wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQGPvKzLQbgr"
      },
      "source": [
        "# Setup\n",
        "\n",
        "Let's setup the openAI key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8BB25JzQcLv"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "openai_api_key = 'API_KEY'\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FZDPvosQkSu"
      },
      "source": [
        "Let's also setup the gpt model, the embedding model and the text splitter for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL8o0HzxQq_N"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0, max_tokens=128)\n",
        "embedding_model =  OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyW3wQBnLtgX"
      },
      "source": [
        "# Langchain WikiWrapper\n",
        "Let's start by using the Langchain wikipedia LLM wrapper to figure out which wikipedia page relates the most with out question.\n",
        "More can be read about the wrapper [here](https://api.python.langchain.com/en/latest/utilities/langchain_community.utilities.wikipedia.WikipediaAPIWrapper.html)\n",
        "\n",
        "\n",
        "This works exactly like we did with the structuring model. We need to create a class that explains what we're looking for and let an LLM do the heavy lifting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsmLZ2arMBqZ"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools import WikipediaQueryRun\n",
        "from langchain_community.utilities import WikipediaAPIWrapper\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "\n",
        "\n",
        "# first we use a class to explain the scheme of the output we're looking for\n",
        "class WikiInputs(BaseModel):\n",
        "    \"\"\"Inputs to the wikipedia tool.\"\"\"\n",
        "\n",
        "    query: str = Field(\n",
        "        description=\"query to look up in Wikipedia, should be 3 or less words\"\n",
        "    )\n",
        "\n",
        "api_wrapper = WikipediaAPIWrapper(top_k_results=3, doc_content_chars_max=100) # we create the langchain api_wrapper object\n",
        "wiki_tool = WikipediaQueryRun( # then we use this class to create the tool we'll be using to make the LLM calls\n",
        "    name=\"wiki-tool\",\n",
        "    description=\"look up things in wikipedia\",\n",
        "    args_schema=WikiInputs, # this uses the schema we just created\n",
        "    api_wrapper=api_wrapper, # this uses the wrapper above\n",
        "    return_direct=True,\n",
        ")\n",
        "\n",
        "# let's take a look at how the output looks like\n",
        "wiki = wiki_tool.run(\"Which interface do people use with code to make requests to the internet?\") # can also be used as wiki_tool.invoke\n",
        "print(wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PelbzohWM3tm"
      },
      "source": [
        "The output here is entirely a string.\n",
        "We can use regular expressions to extract the page title or split on new lines to get each text separately but let us use the structuring model to do that for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbQBahNWNExa"
      },
      "outputs": [],
      "source": [
        "class StructureAPI(BaseModel):\n",
        "  \"\"\"Structure of the explaination\"\"\"\n",
        "  page: str = Field(description=\"The page explained in the output\")\n",
        "  summary: str = Field(description=\"The summary explained by the output\")\n",
        "\n",
        "structured_model =  model.with_structured_output(StructureAPI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMkHx4luNUfe"
      },
      "outputs": [],
      "source": [
        "structured_output = structured_model.invoke(wiki)\n",
        "print(structured_output.page, structured_output.summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ampwokvmNNks"
      },
      "source": [
        "Now let's create a chain that takes in the question and returns this structured output, remember that a chain uses the `|` operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ejrt0LnqNrzJ"
      },
      "outputs": [],
      "source": [
        "wiki_chain = wiki_tool | structured_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFwbhfbON9R5"
      },
      "source": [
        "# Web Scrapping\n",
        "\n",
        "Now that we have a chain to generate the wikipedia page. We can use what we learned from the web scrapping hands-on to scrape that particular wikipedia page and return all its text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JihOPkP8OUbx"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "\n",
        "def wikipedia_search(page: str):\n",
        "  \"\"\"Uses the page name to scrape all the text from the wikipedia page of that title\"\"\"\n",
        "\n",
        "  scrape_url = f' https://en.wikipedia.org/wiki/{page}' # the page is what we get from the chain\n",
        "\n",
        "  response = requests.get(scrape_url) # we do and http request\n",
        "\n",
        "  # Check the response status code.\n",
        "  if(response.status_code == 200):\n",
        "    # Parse the HTML content of the webpage using beautiful soup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    # get all text content from the website\n",
        "    # We will extract the text within the <p> tags which usually contains the main content\n",
        "    text_content = ''\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        text_content += paragraph.get_text()\n",
        "    # returns the text output\n",
        "    return text_content\n",
        "  else:\n",
        "    return \"Couldn't find the page you're looking for\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BmL1oynOh_q"
      },
      "outputs": [],
      "source": [
        "wiki = wiki_chain.invoke(\"Which interface do people use with code to make requests to the internet?\")\n",
        "wiki_article = wikipedia_search(wiki.page)\n",
        "print(wiki_article)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6Ugpi2WO0We"
      },
      "source": [
        "Now we can leave it as that. But we can do something better.\n",
        "Let's make this function into a tool that we can then add as a node to our chain.\n",
        "This will make us only call one chain rather than to the sequential calls manually."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wfrmWxrS1MO"
      },
      "outputs": [],
      "source": [
        "def wikipedia_search_tool(structured_wiki: StructureAPI): # instead of getting the page as string we recieve the output of the previous model which is the structureAPI object\n",
        "  \"\"\"Uses the page name to scrape all the text from the wikipedia page of that title\"\"\"\n",
        "\n",
        "  scrape_url = f' https://en.wikipedia.org/wiki/{structured_wiki.page}' # We will be getting the entire structure object so we need to get the page out of it.\n",
        "\n",
        "  response = requests.get(scrape_url) # we do and http request\n",
        "\n",
        "  # Check the response status code.\n",
        "  if(response.status_code == 200):\n",
        "    # Parse the HTML content of the webpage using beautiful soup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    # get all text content from the website\n",
        "    # We will extract the text within the <p> tags which usually contains the main content\n",
        "    text_content = ''\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        text_content += paragraph.get_text()\n",
        "    # returns the text output\n",
        "    return text_content\n",
        "  else:\n",
        "    return \"Couldn't find the page you're looking for\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzBKfD2iTk_8"
      },
      "source": [
        "# Tool Creation\n",
        "Now we need to create a tool that will take our function and call it as the next step in the chain.\n",
        "Langchain has all the classes we need already"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTtx3OAeTsh4"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import Tool\n",
        "\n",
        "# Define the tool wrapper\n",
        "class WikipediaScrapeTool(Tool):\n",
        "    def __init__(self, func):\n",
        "      super().__init__(name=\"Wikipedia Scrapper\",func=func, description=\"A tool that scrapes text from wikipedia given page information\")\n",
        "      self.func = func # func here is the function that we aim to use as a tool (wikipedia_search_tool in our case)\n",
        "\n",
        "    def invoke(self, tool_input, config=None, **kwargs): # we then need to create the invoke function that the chain will call for us.\n",
        "      return self.func(tool_input)\n",
        "\n",
        "wiki_scraper = WikipediaScrapeTool(wikipedia_search_tool) # then we create an instance of our tool.\n",
        "\n",
        "# we can already test this out by using wiki_scraper.invoke(wiki) but let's already create a chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxSl7r7OUP5d"
      },
      "outputs": [],
      "source": [
        "scrapper_chain = wiki_chain | wiki_scraper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP0IRNN_UaCn"
      },
      "outputs": [],
      "source": [
        "# let's now test out our new chain\n",
        "wiki_article = scrapper_chain.invoke(\"Which interface do people use with code to make requests to the internet?\")\n",
        "wiki_article"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_tUNSd2UiFC"
      },
      "source": [
        "# Splitting\n",
        "\n",
        "Great. Now that we have a chain that scrapes the internet for us let's start by creating the text splits we'll need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbWtK9_NUyaV"
      },
      "outputs": [],
      "source": [
        "text_chunks = text_splitter.create_documents([wiki_article])\n",
        "\n",
        "print(text_chunks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZBaflrWVHTC"
      },
      "source": [
        "# Embedding and storing\n",
        "\n",
        "Now that we have the chunks we need to vectorize them and store them in our vector database.\n",
        "\n",
        "We will again use chroma for this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1ZxvfacVSZK"
      },
      "outputs": [],
      "source": [
        "# let's create a directory to store our data\n",
        "vectorstore_path = \"chroma/\"\n",
        "try:\n",
        "  os.mkdir(vectorstore_path)\n",
        "except FileExistsError:\n",
        "  !rm -rf chroma/  # remove old database files if any\n",
        "  os.mkdir(vectorstore_path)\n",
        "\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents=text_chunks,\n",
        "    embedding=embedding_model,\n",
        "    persist_directory=vectorstore_path\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXWoC9QjWEnh"
      },
      "source": [
        "# Retrieval\n",
        "\n",
        "Now let's create a retriever LLM that uses the stored vectorstore to answer our question for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJIIVDurWMnG"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    model,\n",
        "    retriever=vectordb.as_retriever()\n",
        ")\n",
        "\n",
        "result = qa_chain({\"query\": \"Which interface do people use with code to make requests to the internet?\"}) # any question related to the topic we scrapped here would also work\n",
        "result[\"result\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IVQcRYFW-bB"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "In this project you learned how to use langchain to create an agent able to answer questions using Wikipedia as a citation source.\n",
        "You should by now have some good understanding on:\n",
        "- API calling and web scrapping\n",
        "- Langchain Tools and function calling\n",
        "- Langchain chains and workflow\n",
        "- Text splitting and Embedding\n",
        "- Retrieval-Augmented Generation\n",
        "\n",
        "\n",
        "## Possible next steps\n",
        "- Use a different source other than wikipedia (you'll need to create your own wrapper or check if Langchain has another one, PS: Langchain has an optimized LLM for API calls)\n",
        "- Create a function that takes in the input question and return the retrieval answer in one go (just needs to copy paste all steps into a function call)\n",
        "- Could optimize by not calling the initial chain if the question's been asked before, or if the answer goes to the same page, use the already saved datastore, less calls = less money used)\n",
        "- Be creative. The limit is what you can come up with\n",
        "- Have fun!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqV-GzR4Z3pr"
      },
      "source": [
        "More code and examples can be found in the following links:\n",
        "- The Langchain Docs: https://python.langchain.com/v0.2/docs/introduction/\n",
        "- Code from previous iterations: https://github.com/michaelnoi/venture_labs_build\n",
        "- Code from this iteration: https://github.com/mostafa-elhaiany/AI-Practitioners-src"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
